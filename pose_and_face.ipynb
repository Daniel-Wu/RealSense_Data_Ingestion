{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Detection and Face Blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pose Detection Model\n",
      "Model Loaded\n",
      "Loading coco topology\n",
      "Initializing pose estimation pipeline\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "\n",
    "print(\"Loading Pose Detection Model\")\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "print(\"Loading coco topology\")\n",
    "def get_topology():\n",
    "    import trt_pose.coco\n",
    "    import json\n",
    "    with open('human_pose.json', 'r') as f:\n",
    "        human_pose = json.load(f)\n",
    "\n",
    "    return trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "topology = get_topology()\n",
    "\n",
    "print(\"Initializing pose estimation pipeline\")\n",
    "#Preprocessing constants\n",
    "WIDTH, HEIGHT = 224, 224\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "#The actual pose estimation\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    image = cv2.resize(image, (WIDTH, HEIGHT))\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "def draw_pose(input_img):\n",
    "    W, H, _ = input_img.shape\n",
    "    cmap, paf = model_trt(preprocess(input_img))\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    \n",
    "    draw_objects(input_img, counts, objects, peaks)\n",
    "    \n",
    "    return cv2.resize(input_img, (H, W))\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coords(input_img, counts, objects, peaks, topology_idxs):\n",
    "        \n",
    "    coords = [(-1, -1)]*len(topology_idxs)\n",
    "    height = input_img.shape[0]\n",
    "    width = input_img.shape[1]\n",
    "    for person in range(int(counts[0])):\n",
    "        obj = objects[0][person]\n",
    "        for i, point in enumerate(topology_idxs):\n",
    "            if int(obj[point]) >= 0:\n",
    "                peak = peaks[0][point][int(obj[point])]\n",
    "                coords[i] = (round(float(peak[1]) * width), round(float(peak[0]) * height))\n",
    "                \n",
    "    return coords\n",
    "                \n",
    "def blur_chest(input_img, counts, objects, peaks):\n",
    "    chest = extract_coords(input_img, counts, objects, peaks, [5, 6])\n",
    "    if chest[0][0] != -1 and chest[1][0] != -1:\n",
    "        delta_x = abs(chest[0][0] - chest[1][0])//2\n",
    "        delta_y = abs(chest[0][1] - chest[1][1])//2\n",
    "        x1 = min(chest[0][0], chest[1][0])\n",
    "        x2 = max(chest[0][0], chest[1][0])\n",
    "        y1 = min(chest[0][1], chest[1][1]) \n",
    "        y2 = max(chest[0][1], chest[1][1]) + delta_x\n",
    "\n",
    "        input_img = blur_box(input_img, x1, y1, x2, y2)\n",
    "        cv2.rectangle(input_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return input_img\n",
    "    \n",
    "    \n",
    "def blur_hips(input_img, counts, objects, peaks):\n",
    "    hips = extract_coords(input_img, counts, objects, peaks, [11, 12])\n",
    "    if hips[0][0] != -1 and hips[1][0] != -1:\n",
    "        delta_x = abs(hips[0][0] - hips[1][0])//2\n",
    "        delta_y = abs(hips[0][1] - hips[1][1])//2\n",
    "        x1 = min(hips[0][0], hips[1][0]) - delta_x\n",
    "        x2 = max(hips[0][0], hips[1][0]) + delta_x\n",
    "        y1 = min(hips[0][1], hips[1][1]) - delta_x//2\n",
    "        y2 = max(hips[0][1], hips[1][1]) + delta_x//2\n",
    "\n",
    "        input_img = blur_box(input_img, x1, y1, x2, y2)\n",
    "        cv2.rectangle(input_img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return input_img\n",
    "        \n",
    "def draw_pose(input_img):\n",
    "    W, H, _ = input_img.shape\n",
    "    cmap, paf = model_trt(preprocess(input_img))\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    \n",
    "    draw_objects(input_img, counts, objects, peaks)\n",
    "    input_img = blur_hips(input_img, counts, objects, peaks)\n",
    "    input_img = blur_chest(input_img, counts, objects, peaks)\n",
    "\n",
    "    return cv2.resize(input_img, (H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mtcnn import TrtMtcnn\n",
    "mtcnn = TrtMtcnn()\n",
    "def blur_box(img, x1, y1, x2, y2):\n",
    "    print(x1, y1, x2, y2, img.shape)\n",
    "    x1 = max(x1, 0)\n",
    "    y1 = max(y1, 0)\n",
    "    x2 = min(x2, img.shape[1])\n",
    "    y2 = min(y2, img.shape[0])\n",
    "    print(x1, y1, x2, y2, img.shape)\n",
    "    ROI = img[y1:y2, x1:x2]\n",
    "    \n",
    "    #Pixellate\n",
    "    ROI = cv2.resize(ROI, (8, 8), interpolation=cv2.INTER_LINEAR)\n",
    "    img[y1:y2, x1:x2] = cv2.resize(ROI, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    #Gaussian Blur\n",
    "    #img[y1:y2, x1:x2] = cv2.GaussianBlur(ROI, (51, 51), 0)\n",
    "    return img\n",
    "\n",
    "def blur_faces(img, boxes):\n",
    "    for bb in boxes:\n",
    "        x1, y1, x2, y2 = int(bb[0]), int(bb[1]), int(bb[2]), int(bb[3])\n",
    "        img = blur_box(img, x1, y1, x2, y2)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 314 520 388 (480, 640, 3)\n",
      "259 314 520 388 (480, 640, 3)\n",
      "229 48 404 169 (480, 640, 3)\n",
      "229 48 404 169 (480, 640, 3)\n",
      "274 323 463 371 (480, 640, 3)\n",
      "274 323 463 371 (480, 640, 3)\n",
      "252 94 382 160 (480, 640, 3)\n",
      "252 94 382 160 (480, 640, 3)\n",
      "234 118 277 149 (480, 640, 3)\n",
      "234 118 277 149 (480, 640, 3)\n",
      "188 0 253 76 (480, 640, 3)\n",
      "188 0 253 76 (480, 640, 3)\n",
      "116 338 269 376 (480, 640, 3)\n",
      "116 338 269 376 (480, 640, 3)\n",
      "127 155 255 219 (480, 640, 3)\n",
      "127 155 255 219 (480, 640, 3)\n",
      "162 20 221 96 (480, 640, 3)\n",
      "162 20 221 96 (480, 640, 3)\n",
      "87 340 243 383 (480, 640, 3)\n",
      "87 340 243 383 (480, 640, 3)\n",
      "88 167 214 237 (480, 640, 3)\n",
      "88 167 214 237 (480, 640, 3)\n",
      "135 37 190 110 (480, 640, 3)\n",
      "135 37 190 110 (480, 640, 3)\n",
      "136 343 241 376 (480, 640, 3)\n",
      "136 343 241 376 (480, 640, 3)\n",
      "102 164 213 225 (480, 640, 3)\n",
      "102 164 213 225 (480, 640, 3)\n",
      "144 45 198 116 (480, 640, 3)\n",
      "144 45 198 116 (480, 640, 3)\n",
      "119 336 267 373 (480, 640, 3)\n",
      "119 336 267 373 (480, 640, 3)\n",
      "123 168 237 231 (480, 640, 3)\n",
      "123 168 237 231 (480, 640, 3)\n",
      "168 51 222 120 (480, 640, 3)\n",
      "168 51 222 120 (480, 640, 3)\n",
      "155 333 280 369 (480, 640, 3)\n",
      "155 333 280 369 (480, 640, 3)\n",
      "160 170 261 226 (480, 640, 3)\n",
      "160 170 261 226 (480, 640, 3)\n",
      "202 59 251 126 (480, 640, 3)\n",
      "202 59 251 126 (480, 640, 3)\n",
      "167 348 295 383 (480, 640, 3)\n",
      "167 348 295 383 (480, 640, 3)\n",
      "177 165 280 227 (480, 640, 3)\n",
      "177 165 280 227 (480, 640, 3)\n",
      "221 54 272 121 (480, 640, 3)\n",
      "221 54 272 121 (480, 640, 3)\n",
      "175 337 328 383 (480, 640, 3)\n",
      "175 337 328 383 (480, 640, 3)\n",
      "186 153 300 213 (480, 640, 3)\n",
      "186 153 300 213 (480, 640, 3)\n",
      "224 31 277 97 (480, 640, 3)\n",
      "224 31 277 97 (480, 640, 3)\n",
      "190 324 374 371 (480, 640, 3)\n",
      "190 324 374 371 (480, 640, 3)\n",
      "188 132 318 201 (480, 640, 3)\n",
      "188 132 318 201 (480, 640, 3)\n",
      "223 0 290 66 (480, 640, 3)\n",
      "223 0 290 66 (480, 640, 3)\n",
      "236 314 440 374 (480, 640, 3)\n",
      "236 314 440 374 (480, 640, 3)\n",
      "227 121 382 200 (480, 640, 3)\n",
      "227 121 382 200 (480, 640, 3)\n",
      "301 307 506 373 (480, 640, 3)\n",
      "301 307 506 373 (480, 640, 3)\n",
      "292 88 449 168 (480, 640, 3)\n",
      "292 88 449 168 (480, 640, 3)\n",
      "273 317 526 384 (480, 640, 3)\n",
      "273 317 526 384 (480, 640, 3)\n",
      "289 50 461 148 (480, 640, 3)\n",
      "289 50 461 148 (480, 640, 3)\n",
      "192 4 412 118 (480, 640, 3)\n",
      "192 4 412 118 (480, 640, 3)\n",
      "164 9 383 131 (480, 640, 3)\n",
      "164 9 383 131 (480, 640, 3)\n",
      "152 277 436 372 (480, 640, 3)\n",
      "152 277 436 372 (480, 640, 3)\n",
      "185 9 368 121 (480, 640, 3)\n",
      "185 9 368 121 (480, 640, 3)\n",
      "228 309 485 378 (480, 640, 3)\n",
      "228 309 485 378 (480, 640, 3)\n",
      "232 58 411 160 (480, 640, 3)\n",
      "232 58 411 160 (480, 640, 3)\n",
      "264 311 501 377 (480, 640, 3)\n",
      "264 311 501 377 (480, 640, 3)\n",
      "259 82 437 177 (480, 640, 3)\n",
      "259 82 437 177 (480, 640, 3)\n",
      "263 316 504 377 (480, 640, 3)\n",
      "263 316 504 377 (480, 640, 3)\n",
      "273 91 448 187 (480, 640, 3)\n",
      "273 91 448 187 (480, 640, 3)\n",
      "268 318 501 377 (480, 640, 3)\n",
      "268 318 501 377 (480, 640, 3)\n",
      "271 92 445 186 (480, 640, 3)\n",
      "271 92 445 186 (480, 640, 3)\n",
      "255 317 500 392 (480, 640, 3)\n",
      "255 317 500 392 (480, 640, 3)\n",
      "266 89 438 178 (480, 640, 3)\n",
      "266 89 438 178 (480, 640, 3)\n",
      "253 316 497 393 (480, 640, 3)\n",
      "253 316 497 393 (480, 640, 3)\n",
      "255 79 427 171 (480, 640, 3)\n",
      "255 79 427 171 (480, 640, 3)\n",
      "219 309 468 377 (480, 640, 3)\n",
      "219 309 468 377 (480, 640, 3)\n",
      "225 56 420 161 (480, 640, 3)\n",
      "225 56 420 161 (480, 640, 3)\n",
      "163 295 432 362 (480, 640, 3)\n",
      "163 295 432 362 (480, 640, 3)\n",
      "143 4 386 150 (480, 640, 3)\n",
      "143 4 386 150 (480, 640, 3)\n",
      "81 4 375 151 (480, 640, 3)\n",
      "81 4 375 151 (480, 640, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-c774c3af61af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Align the depth frame to color frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0maligned_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Get aligned frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Camera streaming\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "\n",
    "# Create an align object\n",
    "align_to = rs.stream.color\n",
    "align = rs.align(align_to)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "#Stuff for timing\n",
    "import time\n",
    "start_time = time.time()\n",
    "frame_num = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        depth_frame = aligned_frames.get_depth_frame() # aligned_depth_frame is a 640x480 depth image\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        \n",
    "        #Detect faces\n",
    "        boxes, _ = mtcnn.detect(color_image, minsize=40)\n",
    "        #Do pose estimation\n",
    "        color_image = draw_pose(color_image)\n",
    "        #Blur faces\n",
    "        color_image = blur_faces(color_image, boxes)\n",
    "\n",
    "        #Show image somehow\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        # Stack both images horizontally\n",
    "        images = np.hstack((color_image, depth_colormap))\n",
    "        \n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        cv2.waitKey(1)\n",
    "        \n",
    "        #Timing\n",
    "        frame_num += 1\n",
    "        if frame_num % 100 == 0:\n",
    "            print(\"FPS: Frame\", frame_num, 100/(time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
